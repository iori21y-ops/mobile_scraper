import os, json, asyncio
from datetime import datetime, timezone
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

# í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ ì²˜ë¦¬
TARGETS = [u.strip() for u in os.environ.get("TARGET_URLS", "https://auto.danawa.com/news/").split(",") if u.strip()]
CSS_ITEM  = os.environ.get("CSS_ITEM",  "li")   # ëª©ë¡ ì•„ì´í…œ ì„ íƒì
CSS_TITLE = os.environ.get("CSS_TITLE", "a")    # ì•„ì´í…œ ë‚´ë¶€ ì œëª© ì„ íƒì
CSS_LINK  = os.environ.get("CSS_LINK",  "a")    # ì•„ì´í…œ ë‚´ë¶€ ë§í¬ ì„ íƒì
MAX_ITEMS = int(os.environ.get("MAX_ITEMS", "10"))

async def scrape_url(browser, url: str):
    page = await browser.new_page()

    # í•œêµ­ ì‚¬ì´íŠ¸ ìš°í˜¸ í—¤ë”
    await page.set_extra_http_headers({
        "User-Agent": "Mozilla/5.0",
        "Accept-Language": "ko-KR,ko;q=0.9"
    })

    # ğŸš« ì´ë¯¸ì§€, í°íŠ¸, ìŠ¤íƒ€ì¼ì‹œíŠ¸, ë¯¸ë””ì–´ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
    await page.route("**/*", lambda route:
        route.abort() if route.request.resource_type in ["image", "stylesheet", "font", "media"]
        else route.continue_()
    )

    # í˜ì´ì§€ ì ‘ì† (ë„¤íŠ¸ì›Œí¬ ì•ˆì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
    await page.goto(url, wait_until="domcontentloaded", timeout=45000)

    # í•„ìš”í•œ ìš”ì†Œê°€ ë°˜ë“œì‹œ ë¡œë”©ë˜ë„ë¡ ëª…ì‹œì  ëŒ€ê¸° (ì„ íƒì í•„ìš”ì‹œ ìˆ˜ì •)
    # await page.wait_for_selector(CSS_ITEM, timeout=10000)

    html = await page.content()

    # ğŸ‘‰ HTML ì €ì¥ (ë””ë²„ê¹…/ì…€ë ‰í„° í™•ì¸ìš©)
    safe_name = url.replace("https://", "").replace("/", "_")
    dump_path = f"data/{safe_name}.html"
    os.makedirs("data", exist_ok=True)
    with open(dump_path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"[DEBUG] Saved HTML snapshot: {dump_path}")

    await page.close()

    # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    soup = BeautifulSoup(html, "html.parser")
    out = []
    for li in soup.select(CSS_ITEM)[:MAX_ITEMS]:
        # ì œëª©
        t_el = li.select_one(CSS_TITLE)
        title = (t_el.get_text(strip=True) if t_el else None)

        # ë§í¬
        a_el = li.select_one(CSS_LINK)
        href = a_el.get("href") if a_el else None
        if href and href.startswith("//"):
            href = "https:" + href

        if title or href:
            out.append({"title": title, "url": href})

    return {"source": url, "items": out}

async def main_async():
    data = []
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)

        # ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ëª¨ë“  URL ì²˜ë¦¬
        tasks = [scrape_url(browser, u) for u in TARGETS]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì·¨í•©
        for u, res in zip(TARGETS, results):
            if isinstance(res, Exception):
                data.append({"source": u, "error": str(res)})
            else:
                data.append(res)

        await browser.close()

    payload = {
        "run_at_utc": datetime.now(timezone.utc).isoformat(),
        "count_sources": len(TARGETS),
        "results": data,
    }
    os.makedirs("data", exist_ok=True)
    with open("data/results.json", "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    asyncio.run(main_async())